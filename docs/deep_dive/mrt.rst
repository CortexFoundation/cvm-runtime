
***********
MRT summary
***********

.. contents::

Introduce
=========

MRT, short for **Model Representation Tool**, aims to convert floating model into a deterministi
c and non-data-overflow network. MRT links the off-chain AI developer community to the on-chain e
cosystem, from Off-chain deep learning to MRT transformations, and then uploading to Cortex Block
chain for on-chain deterministic inference.

As the above CVM Runtime section points out, the model that goes
under MRT transformation can be accepted by CVM Runtime, which
we called it on-chain model. MRT propose approaches to transform 
floating model to on-chain model, mainly include:

- do transformation from floating to full integer with minimum 
  accuracy drop.
- certify the process data to be non-flow over INT32.

Application
===========

Quantization steps
------------------

The main quantization procedure is:

    Model Load >>> Preparation >>> [Optional] Model Split >>>
    
    Calibration >>> Quantization >>> [Optional] Model Merge >>> Compilation to CVM,

The execution steps of main.py include prepare, calibrate, quantize, evaluate, and mrt_compile. 
Each step will generate a corresponding intermediate file. You can set the start from that step 
ccording to the parameter COMMON.START_AFTER of the configuration file, 
and use the corresponding intermediate file.

prepare(calibrate, quantize, evaluate, mrt_compile): According to the COMMON and PREPARE(CALIBRATE, QUANTIZE, EVALUATE, MRT_COMPILE) parameters of the configuration file, load the corresponding
model file, call the corresponding function, and save the file generated by the operation to the
model directory. If the split parameter is not empty,
you need to split the model into top and base after prepare, and merge the models after quantize.

Instructions for use
--------------------

Quantization by executing the following code snippet:

.. code-block::

  python main.py config/file/path

The sample configure file is located at ``python/mrt/model_zoo/config.example.ini`` (`config.example.ini <https://github.com/CortexFoundation/cvm-runtime/blob/master/python/mrt/model_zoo/config.example.ini>`_),
and the configure file is located at ``tests/mrt/model_zoo/`` (`model_zoo <https://github.com/CortexFoundation/cvm-runtime/tree/wlt/tests/mrt/model_zoo>`_).

Note
====

Before executing python main.py yaml_file, save the trained model json file and params file to ~/mrt_model/, and the quantized related files are stored in this directory.
About the yaml file, the default value of PREPARE.INPUTSHAPE is [-1, 3, 224, 224], before quantization, you need to set this value to the size of the value input by the model